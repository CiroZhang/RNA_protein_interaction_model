{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b638131-ee12-45bd-9687-9dcfac5afd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "A new version of the following files was downloaded from https://huggingface.co/zhihan1996/DNA_bert_6:\n",
      "- configuration_bert.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/zhihan1996/DNA_bert_6:\n",
      "- dnabert_layer.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 38\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# T5 model\u001b[39;00m\n\u001b[1;32m     33\u001b[0m model_t5 \u001b[38;5;241m=\u001b[39m DNABERTT5Decoder(\n\u001b[1;32m     34\u001b[0m     gene_chunk_nt\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     35\u001b[0m     gene_chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     36\u001b[0m     freeze_gene_encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     37\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 38\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_weights/t5_best.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m model_t5\u001b[38;5;241m.\u001b[39mload_state_dict(state)\n\u001b[1;32m     40\u001b[0m models[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt5\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m model_t5\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:1025\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1024\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1032\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1033\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:1437\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1435\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1436\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1437\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1439\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1440\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1441\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1442\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:1407\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1406\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1407\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:1372\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1370\u001b[0m     storage \u001b[38;5;241m=\u001b[39m overall_storage[storage_offset:storage_offset \u001b[38;5;241m+\u001b[39m numel]\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1372\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;66;03m# swap here if byteswapping is needed\u001b[39;00m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m byteorderdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dataset import read_data\n",
    "from genechat_model import GeneChatModel, DNABERTBartDecoder, DNABERTT5Decoder, DNABERTGRUDecoder\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_data, test_data, _, _ = read_data()\n",
    "\n",
    "# Load all 4 models\n",
    "models = {}\n",
    "\n",
    "# GenChat model\n",
    "model_genechat = GeneChatModel(\n",
    "    gene_chunk_nt=512,\n",
    "    gene_chunk_overlap=0,\n",
    "    freeze_gene_encoder=True,\n",
    ").to(device)\n",
    "state = torch.load(\"model_weights/genechat_best.pt\", map_location=device, weights_only=False)\n",
    "model_genechat.load_state_dict(state)\n",
    "models['genechat'] = model_genechat\n",
    "\n",
    "# BART model\n",
    "model_bart = DNABERTBartDecoder(\n",
    "    gene_chunk_nt=512,\n",
    "    gene_chunk_overlap=0,\n",
    "    freeze_gene_encoder=True,\n",
    ").to(device)\n",
    "state = torch.load(\"model_weights/bart_best.pt\", map_location=device, weights_only=False)\n",
    "model_bart.load_state_dict(state)\n",
    "models['bart'] = model_bart\n",
    "\n",
    "# T5 model\n",
    "model_t5 = DNABERTT5Decoder(\n",
    "    gene_chunk_nt=512,\n",
    "    gene_chunk_overlap=0,\n",
    "    freeze_gene_encoder=True,\n",
    ").to(device)\n",
    "state = torch.load(\"model_weights/t5_best.pt\", map_location=device, weights_only=False)\n",
    "model_t5.load_state_dict(state)\n",
    "models['t5'] = model_t5\n",
    "\n",
    "# GRU model\n",
    "model_gru = DNABERTGRUDecoder(\n",
    "    gene_chunk_nt=512,\n",
    "    gene_chunk_overlap=0,\n",
    "    freeze_gene_encoder=True,\n",
    ").to(device)\n",
    "state = torch.load(\"model_weights/gru_best.pt\", map_location=device, weights_only=False)\n",
    "model_gru.load_state_dict(state)\n",
    "models['gru'] = model_gru\n",
    "\n",
    "print(f\"Loaded {len(models)} models: {list(models.keys())}\")\n",
    "print(f\"Test dataset size: {len(test_data)}\")\n",
    "\n",
    "# Quick test with one example\n",
    "example = test_data[10]\n",
    "dna = example[\"dna\"]\n",
    "target = example[\"target\"]\n",
    "print(f\"\\n=== Ground Truth ===\")\n",
    "print(target[:200])\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    gen = model.generate(\n",
    "        dna=dna,\n",
    "        max_new_tokens=80,\n",
    "        device=device,\n",
    "        temperature=0.8,\n",
    "        top_k=50,\n",
    "    )\n",
    "    print(f\"\\n=== {model_name.upper()} Output ===\")\n",
    "    print(gen[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dcbcce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.3.0a0+40ec155e58.nv24.3)\n",
      "Collecting transformers>=4.30.0 (from -r requirements.txt (line 2))\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn>=1.3.0 (from -r requirements.txt (line 3))\n",
      "  Downloading scikit_learn-1.7.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.66.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (4.10.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (2024.2.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers>=4.30.0->-r requirements.txt (line 2))\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (2.31.0)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.30.0->-r requirements.txt (line 2))\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers>=4.30.0->-r requirements.txt (line 2))\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 3)) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 3)) (3.3.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers>=4.30.0->-r requirements.txt (line 2))\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->-r requirements.txt (line 1)) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.30.0->-r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.30.0->-r requirements.txt (line 2)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.30.0->-r requirements.txt (line 2)) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.30.0->-r requirements.txt (line 2)) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.7.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m193.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.2/507.2 kB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m130.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m130.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, hf-xet, scikit-learn, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.0\n",
      "    Uninstalling scikit-learn-1.2.0:\n",
      "      Successfully uninstalled scikit-learn-1.2.0\n",
      "Successfully installed hf-xet-1.2.0 huggingface-hub-0.36.0 safetensors-0.7.0 scikit-learn-1.7.2 tokenizers-0.22.1 transformers-4.57.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9459545-b4f6-4458-9788-7ae85084383f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet', download_dir='/home/jovyan/nltk_data')\n",
    "nltk.data.path.append('/home/jovyan/nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3718a01a-caad-4959-80e1-09d4ca5be028",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 5\n",
    "random_indices = random.sample(range(len(test_data)), NUM_SAMPLES)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SAMPLE PREDICTIONS - ALL MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx in random_indices:\n",
    "    ex = test_data[idx]\n",
    "    dna = ex[\"dna\"]\n",
    "    ref = ex[\"target\"]\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Sample Index: {idx}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nGround Truth:\\n{ref[:300]}\")\n",
    "    print()\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        pred = model.generate(\n",
    "            dna=dna,\n",
    "            max_new_tokens=80,\n",
    "            device=device,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "        )\n",
    "\n",
    "        m = compute_metrics(ref, pred)\n",
    "        \n",
    "        print(f\"\\n--- {model_name.upper()} ---\")\n",
    "        print(f\"Prediction: {pred[:300]}\")\n",
    "        print(f\"Metrics: BLEU-1={m['bleu1']:.3f}, BLEU-4={m['bleu4']:.3f}, METEOR={m['meteor']:.3f}, ROUGE-1={m['rouge1']:.3f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SAMPLE PREDICTIONS COMPLETE\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cd1fff-4276-4eea-8822-394c1950516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, numpy as np\n",
    "from sacrebleu import BLEU\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# BLEU scorers for BLEU-1..4\n",
    "bleu1 = BLEU(max_ngram_order=1, effective_order=True)\n",
    "bleu2 = BLEU(max_ngram_order=2, effective_order=True)\n",
    "bleu3 = BLEU(max_ngram_order=3, effective_order=True)\n",
    "bleu4 = BLEU(max_ngram_order=4, effective_order=True)\n",
    "\n",
    "# ROUGE scorer\n",
    "rouge = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "def compute_metrics(ref, pred):\n",
    "    \"\"\"Compute BLEU-n, METEOR, ROUGE-1, ROUGE-L.\"\"\"\n",
    "\n",
    "    # BLEU scores\n",
    "    b1 = bleu1.sentence_score(pred, [ref]).score / 100\n",
    "    b2 = bleu2.sentence_score(pred, [ref]).score / 100\n",
    "    b3 = bleu3.sentence_score(pred, [ref]).score / 100\n",
    "    b4 = bleu4.sentence_score(pred, [ref]).score / 100\n",
    "\n",
    "    # METEOR\n",
    "    ref_tok = ref.split()\n",
    "    pred_tok = pred.split()\n",
    "    meteor = meteor_score([ref_tok], pred_tok)\n",
    "\n",
    "    # ROUGE\n",
    "    r = rouge.score(ref, pred)\n",
    "    rouge1 = r[\"rouge1\"].fmeasure\n",
    "    rougeL = r[\"rougeL\"].fmeasure\n",
    "\n",
    "    return {\n",
    "        \"bleu1\": b1,\n",
    "        \"bleu2\": b2,\n",
    "        \"bleu3\": b3,\n",
    "        \"bleu4\": b4,\n",
    "        \"meteor\": meteor,\n",
    "        \"rouge1\": rouge1,\n",
    "        \"rougeL\": rougeL,\n",
    "    }\n",
    "\n",
    "\n",
    "# Storage for ALL models\n",
    "all_model_metrics = {}\n",
    "\n",
    "# Loop through each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Evaluating {model_name.upper()} model...\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Storage lists for this model\n",
    "    bleu1_scores, bleu2_scores, bleu3_scores, bleu4_scores = [], [], [], []\n",
    "    meteor_scores = []\n",
    "    rouge1_scores, rougeL_scores = [], []\n",
    "\n",
    "    # Loop through dataset\n",
    "    for ex in tqdm(test_data, desc=f\"Evaluating {model_name}\"):\n",
    "        dna = ex[\"dna\"]\n",
    "        ref = ex[\"target\"]\n",
    "\n",
    "        pred = model.generate(\n",
    "            dna=dna,\n",
    "            max_new_tokens=80,\n",
    "            device=device,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "        )\n",
    "\n",
    "        m = compute_metrics(ref, pred)\n",
    "\n",
    "        bleu1_scores.append(m[\"bleu1\"])\n",
    "        bleu2_scores.append(m[\"bleu2\"])\n",
    "        bleu3_scores.append(m[\"bleu3\"])\n",
    "        bleu4_scores.append(m[\"bleu4\"])\n",
    "\n",
    "        meteor_scores.append(m[\"meteor\"])\n",
    "\n",
    "        rouge1_scores.append(m[\"rouge1\"])\n",
    "        rougeL_scores.append(m[\"rougeL\"])\n",
    "    \n",
    "    # Store all metrics for this model\n",
    "    all_model_metrics[model_name] = {\n",
    "        'bleu1': bleu1_scores,\n",
    "        'bleu2': bleu2_scores,\n",
    "        'bleu3': bleu3_scores,\n",
    "        'bleu4': bleu4_scores,\n",
    "        'meteor': meteor_scores,\n",
    "        'rouge1': rouge1_scores,\n",
    "        'rougeL': rougeL_scores,\n",
    "    }\n",
    "    \n",
    "    print(f\"Completed evaluation for {model_name}!\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ALL MODELS EVALUATED!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcd3089-60de-4ace-9c4d-35c2e3208d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== COMPUTE AVERAGES FOR ALL MODELS ====\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AVERAGE METRICS COMPARISON - ALL MODELS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "results_summary = {}\n",
    "\n",
    "for model_name, metrics in all_model_metrics.items():\n",
    "    avg_bleu1   = np.mean(metrics['bleu1'])\n",
    "    avg_bleu2   = np.mean(metrics['bleu2'])\n",
    "    avg_bleu3   = np.mean(metrics['bleu3'])\n",
    "    avg_bleu4   = np.mean(metrics['bleu4'])\n",
    "    avg_meteor  = np.mean(metrics['meteor'])\n",
    "    avg_rouge1  = np.mean(metrics['rouge1'])\n",
    "    avg_rougeL  = np.mean(metrics['rougeL'])\n",
    "    \n",
    "    results_summary[model_name] = {\n",
    "        'BLEU-1': avg_bleu1,\n",
    "        'BLEU-2': avg_bleu2,\n",
    "        'BLEU-3': avg_bleu3,\n",
    "        'BLEU-4': avg_bleu4,\n",
    "        'METEOR': avg_meteor,\n",
    "        'ROUGE-1': avg_rouge1,\n",
    "        'ROUGE-L': avg_rougeL,\n",
    "    }\n",
    "    \n",
    "    print(f\"=== {model_name.upper()} ===\")\n",
    "    print(f\"BLEU-1:   {avg_bleu1:.4f}\")\n",
    "    print(f\"BLEU-2:   {avg_bleu2:.4f}\")\n",
    "    print(f\"BLEU-3:   {avg_bleu3:.4f}\")\n",
    "    print(f\"BLEU-4:   {avg_bleu4:.4f}\")\n",
    "    print(f\"METEOR:   {avg_meteor:.4f}\")\n",
    "    print(f\"ROUGE-1:  {avg_rouge1:.4f}\")\n",
    "    print(f\"ROUGE-L:  {avg_rougeL:.4f}\")\n",
    "    print()\n",
    "\n",
    "# ==== CREATE COMPARISON TABLE ====\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results_summary).T\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"METRICS TABLE\")\n",
    "print(\"=\"*80)\n",
    "print(df.to_string())\n",
    "print()\n",
    "\n",
    "# ==== BAR CHART COMPARISON ====\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "fig.suptitle('Metrics Comparison Across All Models', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics_names = ['BLEU-1', 'BLEU-2', 'BLEU-3', 'BLEU-4', 'METEOR', 'ROUGE-1', 'ROUGE-L']\n",
    "model_names = list(all_model_metrics.keys())\n",
    "\n",
    "for idx, metric_name in enumerate(metrics_names):\n",
    "    ax = axes[idx // 4, idx % 4]\n",
    "    values = [results_summary[m][metric_name] for m in model_names]\n",
    "    bars = ax.bar(model_names, values, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'][:len(model_names)])\n",
    "    ax.set_title(metric_name, fontweight='bold')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_ylim(0, max(values) * 1.2)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Hide the last subplot (we have 7 metrics, 8 subplots)\n",
    "axes[1, 3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==== DISTRIBUTION HISTOGRAMS FOR EACH MODEL ====\n",
    "for model_name, metrics in all_model_metrics.items():\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
    "    fig.suptitle(f'{model_name.upper()} - Metric Distributions', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Row 1 — BLEU scores\n",
    "    axes[0, 0].hist(metrics['bleu1'], bins=30, color='steelblue', alpha=0.7)\n",
    "    axes[0, 0].set_title(\"BLEU-1\")\n",
    "    axes[0, 0].axvline(np.mean(metrics['bleu1']), color='red', linestyle='--', label=f'Mean: {np.mean(metrics[\"bleu1\"]):.3f}')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    axes[0, 1].hist(metrics['bleu2'], bins=30, color='steelblue', alpha=0.7)\n",
    "    axes[0, 1].set_title(\"BLEU-2\")\n",
    "    axes[0, 1].axvline(np.mean(metrics['bleu2']), color='red', linestyle='--', label=f'Mean: {np.mean(metrics[\"bleu2\"]):.3f}')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    axes[0, 2].hist(metrics['bleu3'], bins=30, color='steelblue', alpha=0.7)\n",
    "    axes[0, 2].set_title(\"BLEU-3\")\n",
    "    axes[0, 2].axvline(np.mean(metrics['bleu3']), color='red', linestyle='--', label=f'Mean: {np.mean(metrics[\"bleu3\"]):.3f}')\n",
    "    axes[0, 2].legend()\n",
    "    \n",
    "    # Row 2 — BLEU-4, METEOR, ROUGE-1\n",
    "    axes[1, 0].hist(metrics['bleu4'], bins=30, color='steelblue', alpha=0.7)\n",
    "    axes[1, 0].set_title(\"BLEU-4\")\n",
    "    axes[1, 0].axvline(np.mean(metrics['bleu4']), color='red', linestyle='--', label=f'Mean: {np.mean(metrics[\"bleu4\"]):.3f}')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    axes[1, 1].hist(metrics['meteor'], bins=30, color='green', alpha=0.7)\n",
    "    axes[1, 1].set_title(\"METEOR\")\n",
    "    axes[1, 1].axvline(np.mean(metrics['meteor']), color='red', linestyle='--', label=f'Mean: {np.mean(metrics[\"meteor\"]):.3f}')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    axes[1, 2].hist(metrics['rouge1'], bins=30, color='orange', alpha=0.7)\n",
    "    axes[1, 2].set_title(\"ROUGE-1\")\n",
    "    axes[1, 2].axvline(np.mean(metrics['rouge1']), color='red', linestyle='--', label=f'Mean: {np.mean(metrics[\"rouge1\"]):.3f}')\n",
    "    axes[1, 2].legend()\n",
    "    \n",
    "    # Row 3 — ROUGE-L (centered)\n",
    "    axes[2, 1].hist(metrics['rougeL'], bins=30, color='orange', alpha=0.7)\n",
    "    axes[2, 1].set_title(\"ROUGE-L\")\n",
    "    axes[2, 1].axvline(np.mean(metrics['rougeL']), color='red', linestyle='--', label=f'Mean: {np.mean(metrics[\"rougeL\"]):.3f}')\n",
    "    axes[2, 1].legend()\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    axes[2, 0].axis('off')\n",
    "    axes[2, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f72f39d-fb2b-47b2-a51e-19effebc3434",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
